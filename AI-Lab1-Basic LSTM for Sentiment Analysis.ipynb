{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "##### The necessary preprocessing steps were carried out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "# Importing the relevant libraries,classes,etc\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some more importing\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"pos_amazon_cell_phone_reviews.json\") as f: # reading the file and loading the json file \n",
    "    data = json.load(f)\n",
    "data = data['root']\n",
    "#print(len(data))\n",
    "df_pos = pd.DataFrame(data)\n",
    "summaries_positive = []\n",
    "text_positive = []\n",
    "# for getting the summary and the text data.\n",
    "for i in data:\n",
    "    summaries_positive.append(i['summary'])\n",
    "    text_positive.append(i['text'])\n",
    "label = [1] * 108664   # for creating a label of 1 for positive sentiment\n",
    "df_pos['label'] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_pos2 = df_pos[0:1625] # this is to select half of 3250, which is 3000(training) and 250(testing) which is then merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# same procedure for negative reviews\n",
    "with open(\"neg_amazon_cell_phone_reviews.json\") as f:\n",
    "    data = json.load(f)\n",
    "summaries_negative=[]\n",
    "text_negative=[]\n",
    "data = data['root']\n",
    "df_neg = pd.DataFrame(data)\n",
    "for i in data:\n",
    "    summaries_negative.append(i['summary'])\n",
    "    text_negative.append(i['text'])\n",
    "label = [0] * len(data)\n",
    "df_neg['label'] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_neg2 = df_neg[0:1625] # same step for getting the second half of 3250, which is then split into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df_pos2.append(df_neg2) # merging the dataframes for the train/test split(need to be fed to the function from sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 123 # for consistent split each run\n",
    "X = df.iloc[:,:-1] # selecting the data\n",
    "Y = df.iloc[:,-1] # selecting the label\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.076, random_state=seed) # randomized train/test split of approx 3000/250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for tokenizing and generating the sequence input for the LSTM model. Done seperately for train and test.\n",
    "tokenizer = Tokenizer()\n",
    "summary = list(X_train['summary'])\n",
    "summary_test = list(X_test['summary'])\n",
    "\n",
    "tokenizer.fit_on_texts(summary)\n",
    "vocab = tokenizer.word_index\n",
    "vocab_counts = tokenizer.word_counts\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(summary)\n",
    "sequences_test = tokenizer.texts_to_sequences(summary_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_length = len(max(sequences, key = lambda x: len(x)))\n",
    "len(max(sequences_test, key = lambda x: len(x)))\n",
    "pad_length = 22 # the maximum length of sequence from train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# padding the sequence till maxlength of sequence computed in above cell.\n",
    "new_X_train = pad_sequences(sequences, maxlen = pad_length)\n",
    "new_X_test =  pad_sequences(sequences_test, maxlen = pad_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_Y_train = to_categorical(y_train)\n",
    "new_Y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    " \n",
    "    Only computes a batch-wise average of precision.\n",
    " \n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    " \n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "    Only computes a batch-wise average of recall.\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic LSTM Model\n",
    "#### A basic LSTM acrhitecture was constructed, which used a learned embedding layer that comes with the keras API, that is specific to our example. The embedding layer takes as input the vocabulary size and the size of the output vector, which is a hyperparamter that needs to be tuned. We have have used 32(fairly arbitrary)\n",
    "#### There are 128 hidden units(LSTM cells), this is a hyperparameter that can be tuned.\n",
    "#### The final output layer has 2 units(because we passed the data as a one hot encoding), one for each class negative(0) or positive(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(vocab) + 1, 32, input_length=pad_length)) # the embedding layer as described above\n",
    "model.add(LSTM(128,dropout=0.2,recurrent_dropout=0.2)) # The LSTM hidden units. A dropout was used.\n",
    "model.add(Dense(2,activation='sigmoid')) # 2 sigmoid output units for obtaining a 0 to 1 probability of classification\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', precision, recall]) # precision and recall also calculated for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2522 samples, validate on 481 samples\n",
      "Epoch 1/10\n",
      "2522/2522 [==============================] - 5s - loss: 0.6442 - acc: 0.6689 - precision: 0.6790 - recall: 0.6471 - val_loss: 0.4837 - val_acc: 0.8441 - val_precision: 0.8468 - val_recall: 0.8399\n",
      "Epoch 2/10\n",
      "2522/2522 [==============================] - 4s - loss: 0.3551 - acc: 0.8805 - precision: 0.8801 - recall: 0.8810 - val_loss: 0.3293 - val_acc: 0.8784 - val_precision: 0.8776 - val_recall: 0.8794\n",
      "Epoch 3/10\n",
      "2522/2522 [==============================] - 4s - loss: 0.2286 - acc: 0.9231 - precision: 0.9235 - recall: 0.9227 - val_loss: 0.3092 - val_acc: 0.8867 - val_precision: 0.8875 - val_recall: 0.8857\n",
      "Epoch 4/10\n",
      "2522/2522 [==============================] - 4s - loss: 0.1617 - acc: 0.9508 - precision: 0.9515 - recall: 0.9500 - val_loss: 0.3015 - val_acc: 0.8857 - val_precision: 0.8857 - val_recall: 0.8857\n",
      "Epoch 5/10\n",
      "2522/2522 [==============================] - 4s - loss: 0.1290 - acc: 0.9600 - precision: 0.9600 - recall: 0.9600 - val_loss: 0.3164 - val_acc: 0.8846 - val_precision: 0.8853 - val_recall: 0.8836\n",
      "Epoch 6/10\n",
      "2522/2522 [==============================] - 4s - loss: 0.0931 - acc: 0.9695 - precision: 0.9695 - recall: 0.9695 - val_loss: 0.3299 - val_acc: 0.8898 - val_precision: 0.8898 - val_recall: 0.8898\n",
      "Epoch 7/10\n",
      "2522/2522 [==============================] - 4s - loss: 0.0784 - acc: 0.9768 - precision: 0.9770 - recall: 0.9766 - val_loss: 0.3340 - val_acc: 0.8773 - val_precision: 0.8773 - val_recall: 0.8773\n",
      "Epoch 8/10\n",
      "2522/2522 [==============================] - 4s - loss: 0.0613 - acc: 0.9826 - precision: 0.9826 - recall: 0.9826 - val_loss: 0.3817 - val_acc: 0.8773 - val_precision: 0.8773 - val_recall: 0.8773\n",
      "Epoch 9/10\n",
      "2522/2522 [==============================] - 4s - loss: 0.0574 - acc: 0.9812 - precision: 0.9810 - recall: 0.9814 - val_loss: 0.3587 - val_acc: 0.8867 - val_precision: 0.8864 - val_recall: 0.8877\n",
      "Epoch 10/10\n",
      "2522/2522 [==============================] - 4s - loss: 0.0507 - acc: 0.9843 - precision: 0.9842 - recall: 0.9845 - val_loss: 0.4057 - val_acc: 0.8815 - val_precision: 0.8815 - val_recall: 0.8815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20138f5be48>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(new_X_train, new_Y_train, validation_split = 0.16, epochs = 10) # ran for 10 epochs, with validation/hold out set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.07%\n",
      "Precision: 83.61%\n",
      "Recall: 93.58%\n",
      "F-score: 88.31%\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the test set.\n",
    "scores = model.evaluate(new_X_test, new_Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "y_pred = model.predict(new_X_test)\n",
    "\n",
    "[p, r, f, _] = precision_recall_fscore_support([np.argmax(x) for x in new_Y_test], [np.argmax(x) for x in y_pred], average='binary')\n",
    "\n",
    "print(\"Precision: %.2f%%\" % (p*100))\n",
    "print(\"Recall: %.2f%%\" % (r*100))\n",
    "print(\"F-score: %.2f%%\" % (f*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 22, 32)            65760     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               82432     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 148,450\n",
      "Trainable params: 148,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
